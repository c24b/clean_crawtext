=CRAWTEXT=

== SPECIFICATIONS ==
Crawler pour l'analyse de corpus web.
=>Définition du corpus d'études
- un fichier texte (une url par ligne)
- une requête web (résultats de recherche BING)
- approche combinée ( fichier source + recherche)

=> Définition de la fréquence des crawls
*Définir la fréquence
*Définir le périmêtre
- Discovery: Simple requête web
- Périmêtre défini à une liste de sources fixes


=> Résultats =:
Une base de donnée (exportable en JSON) avec deux tables:
-Résultats: 
	Url de départ, titre, article en texte, urls sortantes
- Erreurs de traitement: 
 Url, code erreur, description
 
 
 == ROADMAP==
> Gestion des erreurs:
 - Traiter l'intégralité des erreurs de téléchargement de page
- Intégrer un rapport sur les urls erronées dans la base de données (table distincte des résultats)
Commentaire: Reste une erreur sur le chargement de certaines pages à voir avec HttpLib et le formattage de la requete
Etat: Terminé

> Base de données:
- Ajout d'un champ nom du porjet pour création d'une BDD correspondante structurée comme suit:
	--Collection: "results" 
	{
	 "url": "www.example.com", 
	 "pointers":["www.example4.com", "www.example5.com","www.example6.com"],
	 "outlinks":["www.example4.com", "www.example5.com","www.example6.com"], 
	 "title": "titre de la page", 
	 "source":"Le texte nettoyé de la page", 
	 "pubdate":timestamp(2005-10-30),
	 "crawdate": timestamp(2005-10-30 T 23:00),
	}
	--Collection: "report"
	{
	 "url": "www.example.com", 
	 "error_code":404, 
	 "error_description": "Page not found",
	}
Ecriture en continu des résultats du crawler
Etat d'avancement : 70%

> Ajout de fréquence et périmêtre pour le crawl
- Fréquence d'execution du crawl défini par l'utilisateur:
3 options : -tous les jours pendant 1 mois 
			-toutes les semaines pendant 1 mois
			- une fois par mois pendant 6 mois

Remarque: là on peut aussi ajouter par heure, par minutes, le lundi mardi jeudi par exemple  qu'on veut 
mais je suis pas sure que ce soit vraiment nécessaire et autant ne pas embrouiller l'utilisateur

- Périmêtre d'execution du crawler:
Mode exploratoire découverte: à chaque execution du craler nouvelle requete sur 



- Alimentation cumulative de la BDD de départ: (mode exploratoire)
- Mode exploratoire


 

● Base de données
● 2 options:
- exploration via api pour alimentation cumulative de la bdd
- premiere exploration via api et bdd source fixe
● Date
● Routine gestion de crawl a date fixe
● Rapport par mail  





 
